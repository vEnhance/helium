\chapter{Instructions for Helium administrator}
Here are some tasks you need to do to set up Helium
to work before the tournament, and after grading.

\section{Database maintenance}
\subsection{Rinse the database}
For posterity, first save another copy of the data using Django management:
\begin{center}
	\texttt{python manage.py dumpdata helium --indent 1}
\end{center}
This will output a file with all Helium related data.
Thus if you ever need it back you can then reload it with \texttt{loaddata}.

Then, one should SSH into the instance and run the following command.
\begin{center}
	\texttt{python manage.py helium\_rinse}
\end{center}
which will clear all Helium-related data.

There is also a command
\begin{center}
	\texttt{python manage.py helium\_dry}
\end{center}
which will clear all grading information entered by graders
and name assignments, but not delete scans or entities.
I added this command before November 2017 because
I wanted to be able to use February 2017 data
to re-simulate the grading experience.
You don't have to use this, though.

\subsection{Import exams from Babbage}
You need to populate the Helium tables with exams.

To do this, you should go to the problem database and for each contest
that you want to export, push the ``Export to Helium'' link.
This will copy all the problems and their answers into Helium,
and then open the admin interface
so that you can make any manual changes necessary
(for example partial marks are disabled by default
but you will need to add them into Guts estimation).
Remember to input colors corresponding to how the answer sheets will be tinted.

A safety feature is that exam names in Helium must be distinct.
So if you decide you want to re-import an exam,
you will have to delete the old one first.

\subsection{Import entities from registration}
In Babbage a command called ``Import Entities from Reg'' will do this for you.
It is careful, and will never import the same entity twice.
This is also the ``heliumimport'' command.

It is very important to \textbf{run heliumimport as late as possible},
preferably just after the first individual exam begins.
Registration always changes last-minute.

\section{Guts estimation problems}
First, ensure that the name of the Guts exam is ``Guts'',
or otherwise maul \texttt{babbage/views.py} to make this work.

Next, if there are any estimation problems the scoring functions
for these need to be added.
You can do this in the admin interface, under ``Guts Score Funcs''.
This should be entirely self-explanatory.

\section{Printing and checking scans}
You should make sure that a large number of answer sheets are printed,
and in different colors between tests to avoid errors.

Unfortunately, right now the scan regions (i.e.\ the cut-outs)
are hard-coded into \verb+scanimage.py+.
So before the tournament you should check the scanners being used
and verify that the cut-out regions are compatible with the scanner.

The file \verb+scanimage.py+ has a facility to make this easier.
If you run
\begin{center}
\verb+python /path/to/scanimage.py FILENAME.pdf+
\end{center}
then it will generate (in the current directory)
the images corresponding to the cut-outs.
Thus you can test this locally and make adjustments to the regions if needed.
\section{Name stickers}
Make sure tournament directors are printing out name stickers for the students;
this will make grading life a lot easier.
Students should be instructed to:
place their name stickers in the field that says ``name'',
but still write their ID number in the blanks provided manually.

Also, on personalized schedules for teams, print out ID numbers.
Doing so will expedite the process of grading significantly.

This task takes place outside of Helium, and varies between tournaments.
The bottom line is that \textbf{ensure students and teams know their ID's}.

\section{Sanity checking results}
Once grading is done, you should do the following clean-up tasks.
\begin{itemize}
	\ii First, open the Verdict table in Django admin and look for verdicts which
	are ``inaccessible''.
	In theory there should be none.
	If such a verdict exists, do a sanity check to make sure it's an anomaly,
	then delete these verdicts.

	\ii If there are verdicts with no names, verify this is correct:
	these should only come about from no-name papers,
	or papers yet to be matched.

	In November 2016 for unknown reasons, some exam scribbles got matched
	but the underlying verdicts for them were left as None.
	You can auto-magically fix these by running the
	management command ``update scribbles''
	which will fix any issues.

	\ii Look in the Entity table for any entities which have
	no verdicts attached to them.
	First delete all individuals with no verdicts:
	this probably means that they did not show up.

	Teams can also not show up,
	but they can also leave early and not take Team or Guts round.
	That's why it's necessary to delete no-show individuals first.
	Only after this, if a team has no verdicts \emph{and} no individuals,
	then one can delete it.
\end{itemize}

\section{Last grading commands}
There are two different management commands at play.
You need to run both, in this order.

\begin{itemize} 
	\ii ``algscore'': this command is slow, taking about two minutes per run.
	It computes the $\alpha$ and $\beta$ values for each student
	based on the set of observations.

	\ii ``grade'': This command is fairly fast, it should take no more than
	a few seconds per run-through.
	It collates together all the results into a table which
	are then displayed in results reports.
	This is necessary because otherwise the process of aggregating the scores
	is too slow.
\end{itemize}

Run both commands, then look through the reports to make sure everything is okay.

It may be wise to run ``grade'' throughout the day,
just to make sure that things are looking okay.
The $\alpha$ and $\beta$ values won't be correct, but that's okay;
we don't need those values when we're just sanity checking results.
\section{Printing results}
The easy part.  After running the ``grade'' command,
the dashboard has several links to the scores in various formats
(OpenDocument spreadsheet, text file, etc),
and an automated awards presentation TeX file.
Congratulations on finishing!
